{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Copy of 09-01-Dimensionality Reduction.ipynb","provenance":[{"file_id":"177LnNZedhkrBQiF3HJwNGLLQvUdh7ZDk","timestamp":1572424379058}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_eAP-Mm_KfI3","colab_type":"text"},"source":["# Dimensionality Reduction"]},{"cell_type":"markdown","metadata":{"id":"foj0G1BtKfI4","colab_type":"text"},"source":["## Principal Component Analysis (PCA)\n","  * Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n","  * [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n","  * Statistical procedure that utilise [orthogonal transformation](https://en.wikipedia.org/wiki/Orthogonal_transformation) technology\n","  * Convert possible correlated features (predictors) into linearly uncorrelated features (predictors) called **principal components**\n","  * \\# of principal components <= number of features (predictors)\n","  * First principal component explains the largest possible variance\n","  * Each subsequent component has the highest variance subject to the restriction that it must be orthogonal to the preceding components. \n","  * A collection of the components are called vectors.\n","  * Sensitive to scaling\n","  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Component axes that maximise the variance"]},{"cell_type":"markdown","metadata":{"id":"L4KN-XRcKfI5","colab_type":"text"},"source":["## Linear Discriminant Analysis (LDA) \n","  * [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n","  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html)\n","  * Most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. \n","  * Goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n","  * Locate the 'boundaries' around clusters of classes.  \n","  * Projects data points on a line\n","  * A centroid will be allocated to each cluster or have a centroid nearby\n","  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Maximising the component axes for class-separation"]},{"cell_type":"markdown","metadata":{"id":"hIwuYWtVKfI5","colab_type":"text"},"source":["### Other Dimensionality Reduction Techniques\n","\n","* [Multidimensional Scaling (MDS) ](http://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds)\n","  * Seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n","\n","\n","* [Isomap (Isometric Mapping)](http://scikit-learn.org/stable/modules/manifold.html#isomap)\n","\n","  * Seeks a lower-dimensional embedding which maintains geodesic distances between all points.\n","\n","\n","* [t-distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n","\n","  * Nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. \n","  * Models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. dimensional space (e.g., to visualize the MNIST images in 2D).\n"]},{"cell_type":"markdown","metadata":{"id":"lAD2WJIVKfI6","colab_type":"text"},"source":["***\n","# Gentle introduction to Linear Algebra"]},{"cell_type":"markdown","metadata":{"id":"jqj8mL0NKfI6","colab_type":"text"},"source":["Linear Algebra revision:\n","\n","$$A=\\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix}$$\n","\n","$$B=\\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix}$$\n","\n","\\begin{align}\n","A \\times B & = \\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix} \\times \\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix} \\\\\n","& = \\begin{bmatrix} 201. & 402. \\\\ 2010. & 4020. \\end{bmatrix} \\\\\n","\\end{align}\n","\n","By parts:\n","$$A \\times B = \\begin{bmatrix} 1. \\times 1. + 2.  \\times 100. &  1. \\times 2. + 2. \\times 200. \\\\ \n","10. \\times 1. + 20. \\times 100. & 10. \\times 2. + 20. \\times 200. \\end{bmatrix}$$\n"]},{"cell_type":"markdown","metadata":{"id":"9FL0CNAOKfI7","colab_type":"text"},"source":["### Libraries"]},{"cell_type":"code","metadata":{"id":"gJktEcimKfI8","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLDrgfdfKfI-","colab_type":"code","colab":{}},"source":["A = [[1., 2.], [10., 20.]]\n","B = [[1., 2.], [100., 200.]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HdUKLAzKfJA","colab_type":"code","outputId":"94ee2e50-7a07-4a84-9ab8-3aad248ed7f1","executionInfo":{"status":"ok","timestamp":1571231992175,"user_tz":-480,"elapsed":1225,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"01122201020865534731"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["A"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1.0, 2.0], [10.0, 20.0]]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"7DQu_xwPKfJB","colab_type":"code","outputId":"9a47504e-1a40-4a69-b67b-52ba212989d4","executionInfo":{"status":"ok","timestamp":1571231992177,"user_tz":-480,"elapsed":1218,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"01122201020865534731"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["B"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1.0, 2.0], [100.0, 200.0]]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"NKu3c4kDKfJE","colab_type":"code","outputId":"8d28dead-3fcc-4ae5-fc68-357689920e4a","executionInfo":{"status":"ok","timestamp":1571231998422,"user_tz":-480,"elapsed":896,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"01122201020865534731"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["np.dot(A, B)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 201.,  402.],\n","       [2010., 4020.]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"1H3ibswyKfJG","colab_type":"text"},"source":["***"]}]}